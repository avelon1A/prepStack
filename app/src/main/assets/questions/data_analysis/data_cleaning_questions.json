{
  "topicId": "data_cleaning",
  "questions": [
    {"id": "data_cleaning_theory_1", "questionText": "What is data cleaning and why is it important?", "type": "THEORY", "correctAnswer": "Data cleaning is the process of identifying and correcting errors, inconsistencies, and inaccuracies in datasets. It's important because poor data quality leads to unreliable analysis results, incorrect insights, and flawed decision-making.", "explanation": "The 'garbage in, garbage out' principle applies to data analysis – even sophisticated methods cannot produce reliable results from messy data.", "difficulty": "EASY"},
    
    {"id": "data_cleaning_theory_2", "questionText": "What are common strategies for handling missing values?", "type": "THEORY", "correctAnswer": "Common strategies include: 1) Deletion (removing rows or columns with missing data), 2) Imputation (filling missing values with mean, median, mode, or predicted values), 3) Using algorithms that handle missing values, 4) Creating a 'missing' category, and 5) Using interpolation for time series data.", "explanation": "The best approach depends on the amount and pattern of missing data, the analysis goals, and the specific domain. Improper handling can introduce bias or reduce statistical power.", "codeExample": "# Python example - Different imputation methods\n\n# Mean imputation\ndf['column'].fillna(df['column'].mean(), inplace=True)\n\n# Median imputation\ndf['column'].fillna(df['column'].median(), inplace=True)\n\n# Mode imputation\ndf['column'].fillna(df['column'].mode()[0], inplace=True)\n\n# Forward fill (for time series)\ndf['column'].fillna(method='ffill', inplace=True)", "difficulty": "MEDIUM"},
    
    {"id": "data_cleaning_theory_3", "questionText": "How do you identify and handle outliers in a dataset?", "type": "THEORY", "correctAnswer": "Outliers can be identified using: 1) Visual methods (box plots, scatter plots), 2) Statistical methods (z-scores, IQR method, Mahalanobis distance), and 3) Domain expertise. Handling options include: removing them, transforming the data, capping at thresholds, or treating them separately in analysis.", "explanation": "Not all unusual values are errors – some represent important edge cases or phenomena of interest. The decision to remove or retain outliers should consider their cause and the analysis objectives.", "codeExample": "# Python example - Identifying outliers\n\n# Z-score method\nimport numpy as np\nfrom scipy import stats\nz_scores = np.abs(stats.zscore(df['column']))\noutliers = df[z_scores > 3]  # Values more than 3 standard deviations away\n\n# IQR method\nQ1 = df['column'].quantile(0.25)\nQ3 = df['column'].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\noutliers = df[(df['column'] < lower_bound) | (df['column'] > upper_bound)]", "difficulty": "HARD"},
    
    {"id": "data_cleaning_theory_4", "questionText": "What is data normalization and when should you use it?", "type": "THEORY", "correctAnswer": "Data normalization is the process of rescaling numeric features to a standard range (typically 0-1 or -1 to 1). It should be used when: 1) Features have different scales, 2) Using distance-based algorithms (k-means, kNN), 3) Using gradient descent-based algorithms, and 4) When comparing measurements with different units.", "explanation": "Normalization prevents features with larger scales from dominating the analysis and ensures all features contribute equally to the result. Common methods include min-max scaling and z-score standardization.", "codeExample": "# Python example - Normalization techniques\n\n# Min-Max scaling (0 to 1 range)\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf_normalized = scaler.fit_transform(df[['col1', 'col2']])\n\n# Z-score standardization (mean=0, std=1)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf_standardized = scaler.fit_transform(df[['col1', 'col2']])", "difficulty": "MEDIUM"},
    
    {"id": "data_cleaning_theory_5", "questionText": "What is data deduplication and what techniques can be used for it?", "type": "THEORY", "correctAnswer": "Data deduplication is the process of identifying and removing duplicate records. Techniques include: 1) Exact matching on unique identifiers, 2) Fuzzy matching for text with slight variations, 3) Record linkage using multiple fields, and 4) Phonetic matching for names with similar pronunciations.", "explanation": "Duplicates can skew analysis results, especially in counting or summarization operations. The challenge often lies in identifying non-exact duplicates due to typos, format differences, or partial information.", "codeExample": "# Python example - Deduplication methods\n\n# Exact duplicates\ndf_no_duplicates = df.drop_duplicates()\n\n# Duplicates based on specific columns\ndf_no_duplicates = df.drop_duplicates(subset=['first_name', 'last_name', 'dob'])\n\n# Fuzzy matching with difflib\nimport difflib\ndef fuzzy_match(str1, str2, threshold=0.9):\n    similarity = difflib.SequenceMatcher(None, str1.lower(), str2.lower()).ratio()\n    return similarity >= threshold", "difficulty": "HARD"}
  ]
}