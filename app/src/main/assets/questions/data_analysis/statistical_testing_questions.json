{
  "topicId": "statistical_testing",
  "questions": [
    {"id": "stat_test_theory_1", "questionText": "What is a hypothesis test and what are the null and alternative hypotheses?", "type": "THEORY", "correctAnswer": "A hypothesis test is a statistical method to determine whether observed data supports a particular claim. The null hypothesis (H₀) is the default position that there is no effect or relationship. The alternative hypothesis (H₁ or Hₐ) is the claim being tested, typically that there is some effect or relationship.", "explanation": "Hypothesis tests follow a process of assuming the null is true, calculating the probability of observing results at least as extreme as those observed, and making a decision based on a predefined significance level.", "difficulty": "EASY"},
    
    {"id": "stat_test_theory_2", "questionText": "Explain the concept of p-value and its interpretation in hypothesis testing.", "type": "THEORY", "correctAnswer": "The p-value is the probability of obtaining results at least as extreme as those observed, assuming the null hypothesis is true. A small p-value (typically ≤ 0.05) indicates that the observed data would be unlikely under the null hypothesis, providing evidence to reject it in favor of the alternative hypothesis.", "explanation": "The p-value does NOT represent the probability that the null hypothesis is true or false, nor does it indicate the size or importance of an effect. It simply measures the compatibility between the observed data and the null hypothesis.", "difficulty": "MEDIUM"},
    
    {"id": "stat_test_theory_3", "questionText": "What is the difference between Type I and Type II errors in hypothesis testing?", "type": "THEORY", "correctAnswer": "Type I error (false positive): Rejecting the null hypothesis when it is actually true. The probability of a Type I error is equal to the significance level (α).\nType II error (false negative): Failing to reject the null hypothesis when it is actually false. The probability of a Type II error is denoted as β, and power equals 1 - β.", "explanation": "There is a trade-off between these error types. Decreasing the significance level (α) reduces Type I errors but increases Type II errors. Increasing sample size can reduce both types of errors simultaneously.", "difficulty": "MEDIUM"},
    
    {"id": "stat_test_theory_4", "questionText": "What is the difference between parametric and non-parametric statistical tests?", "type": "THEORY", "correctAnswer": "Parametric tests make assumptions about the population parameters and distribution of the data (typically normal distribution). Examples include t-tests, ANOVA, and linear regression. Non-parametric tests don't assume a specific distribution and are more flexible but often less powerful. Examples include Mann-Whitney U, Kruskal-Wallis, and Spearman's rank correlation.", "explanation": "Use parametric tests when assumptions are met for more statistical power. Use non-parametric tests when assumptions are violated, working with ordinal data, or with small samples where normality cannot be verified.", "difficulty": "HARD"},
    
    {"id": "stat_test_theory_5", "questionText": "What is statistical power and what factors affect it?", "type": "THEORY", "correctAnswer": "Statistical power is the probability of correctly rejecting the null hypothesis when it is false (detecting a true effect). Factors that increase power include: 1) Larger sample sizes, 2) Larger effect sizes, 3) Lower variability in the data, 4) Higher significance levels (though this increases Type I error rate), and 5) Appropriate test selection for the data.", "explanation": "Low power increases the risk of Type II errors (missing real effects) and can lead to publication bias when only significant results are published. Power analysis before conducting a study helps determine adequate sample sizes.", "difficulty": "HARD"}
  ]
}