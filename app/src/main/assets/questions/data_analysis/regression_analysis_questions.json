{
  "topicId": "regression_analysis",
  "questions": [
    {"id": "regression_theory_1", "questionText": "What is regression analysis and what are its main objectives?", "type": "THEORY", "correctAnswer": "Regression analysis is a statistical method for examining relationships between a dependent variable and one or more independent variables. The main objectives include: 1) Identifying relationships between variables, 2) Predicting the dependent variable based on independent variables, 3) Quantifying the strength of these relationships, and 4) Determining which independent variables have the strongest influence on the dependent variable.", "explanation": "Regression models can range from simple linear models with one predictor to complex models with multiple predictors and non-linear relationships.", "difficulty": "EASY"},
    
    {"id": "regression_theory_2", "questionText": "What is the difference between simple linear regression and multiple linear regression?", "type": "THEORY", "correctAnswer": "Simple linear regression examines the relationship between one independent variable and one dependent variable, using the equation y = β₀ + β₁x + ε. Multiple linear regression includes two or more independent variables, using the equation y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε, allowing for the analysis of more complex relationships.", "explanation": "Multiple regression is an extension of simple regression that allows for controlling for confounding variables and assessing the unique contribution of each independent variable while holding others constant.", "codeExample": "# Python example\nimport statsmodels.api as sm\n\n# Simple linear regression\nX = df[['income']]  # One predictor\nX = sm.add_constant(X)  # Add intercept term\nmodel = sm.OLS(df['spending'], X).fit()\nprint(model.summary())\n\n# Multiple linear regression\nX = df[['income', 'age', 'credit_score']]  # Multiple predictors\nX = sm.add_constant(X)\nmodel = sm.OLS(df['spending'], X).fit()\nprint(model.summary())", "difficulty": "EASY"},
    
    {"id": "regression_theory_3", "questionText": "What are the key assumptions of linear regression and how can you check them?", "type": "THEORY", "correctAnswer": "The key assumptions are: 1) Linearity: the relationship between X and Y is linear, 2) Independence: observations are independent of each other, 3) Homoscedasticity: error variance is constant across all levels of X, 4) Normality: errors are normally distributed, and 5) No multicollinearity: independent variables are not highly correlated. These can be checked through residual plots, Q-Q plots, Durbin-Watson tests, and variance inflation factors.", "explanation": "Violating these assumptions doesn't necessarily invalidate the analysis but can lead to biased estimates, incorrect standard errors, or inappropriate inferences. Transformations or different models might be needed when assumptions are violated.", "codeExample": "# Python example for checking assumptions\n\n# Get residuals\nresiduals = model.resid\npredicted = model.predict()\n\n# Linearity and homoscedasticity\nimport matplotlib.pyplot as plt\nplt.scatter(predicted, residuals)\nplt.axhline(y=0, color='r')\nplt.title('Residuals vs Predicted')\nplt.show()\n\n# Normality of residuals\nimport scipy.stats as stats\nstats.probplot(residuals, plot=plt)\nplt.title('Q-Q Plot')\nplt.show()\n\n# Test for autocorrelation\nfrom statsmodels.stats.stattools import durbin_watson\ndw_stat = durbin_watson(residuals)\nprint(f\"Durbin-Watson statistic: {dw_stat}\")", "difficulty": "MEDIUM"},
    
    {"id": "regression_theory_4", "questionText": "What are the differences between linear and logistic regression?", "type": "THEORY", "correctAnswer": "Linear regression predicts a continuous dependent variable and uses the least squares method to find the best fit line. Logistic regression predicts a binary outcome (probability between 0 and 1) and uses maximum likelihood estimation. Linear regression assumes a linear relationship and normal distribution of errors, while logistic regression uses the logistic function and doesn't require these assumptions.", "explanation": "While both are regression methods, they serve different purposes: linear regression for predicting quantities, logistic regression for predicting probabilities of binary outcomes like yes/no, success/failure, present/absent.", "difficulty": "MEDIUM"},
    
    {"id": "regression_theory_5", "questionText": "What is regularization in regression and when would you use it?", "type": "THEORY", "correctAnswer": "Regularization is a technique that adds a penalty term to the cost function to prevent overfitting by reducing the magnitude of coefficient values. Common methods include Ridge regression (L2 penalty), Lasso regression (L1 penalty), and Elastic Net (combination of L1 and L2). Use regularization when dealing with many predictors, multicollinearity, or when you suspect overfitting in your model.", "explanation": "Ridge tends to shrink coefficients toward zero but rarely eliminates them, while Lasso can drive coefficients to exactly zero, effectively performing feature selection. Elastic Net combines these properties.", "codeExample": "# Python example of regularization\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\n\n# Ridge regression (L2 penalty)\nridge = Ridge(alpha=1.0)  # alpha controls regularization strength\nridge.fit(X_train, y_train)\n\n# Lasso regression (L1 penalty)\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train, y_train)\n\n# Elastic Net (L1 + L2 penalty)\nelastic = ElasticNet(alpha=0.1, l1_ratio=0.5)  # l1_ratio controls the mix\nelastic.fit(X_train, y_train)", "difficulty": "HARD"}
  ]
}