{
  "topicId": "data_cleaning",
  "quizQuestions": [
    {"id": "data_cleaning_quiz_1", "questionText": "Which of the following is NOT a common technique for handling missing values?", "type": "MCQ", "options": ["Randomization", "Mean imputation", "Listwise deletion", "Multiple imputation"], "correctAnswer": "Randomization", "explanation": "While mean imputation (replacing with averages), listwise deletion (removing rows with missing values), and multiple imputation (using statistical models to estimate values) are standard approaches, randomly generating values for missing data is generally not recommended as it introduces noise without statistical basis.", "difficulty": "EASY"},
    
    {"id": "data_cleaning_quiz_2", "questionText": "In the context of outlier detection, what does the IQR method use to identify outliers?", "type": "MCQ", "options": ["Values beyond 1.5 × IQR from the quartiles", "Values more than 3 standard deviations from the mean", "Values more than 5% different from the median", "Values that appear less than 1% of the time"], "correctAnswer": "Values beyond 1.5 × IQR from the quartiles", "explanation": "The Interquartile Range (IQR) method defines outliers as values that fall below Q1 - 1.5 × IQR or above Q3 + 1.5 × IQR, where Q1 and Q3 are the first and third quartiles, and IQR is the difference between them.", "difficulty": "MEDIUM"},
    
    {"id": "data_cleaning_quiz_3", "questionText": "Which data transformation is most effective for right-skewed data with a positive skew?", "type": "MCQ", "options": ["Log transformation", "Square transformation", "Inverse transformation", "Z-score standardization"], "correctAnswer": "Log transformation", "explanation": "Log transformations compress higher values more than lower values, making them particularly effective for right-skewed (positively skewed) distributions, which have a long tail of high values that need to be brought closer to the center.", "difficulty": "MEDIUM"},
    
    {"id": "data_cleaning_quiz_4", "questionText": "What is the primary purpose of data normalization?", "type": "MCQ", "options": ["To bring features to comparable scales", "To remove missing values", "To eliminate outliers", "To reduce the number of features"], "correctAnswer": "To bring features to comparable scales", "explanation": "The main purpose of normalization is to rescale numeric features so they have comparable ranges, preventing features with larger scales from dominating the analysis and ensuring all features contribute proportionally to the results.", "difficulty": "EASY"},
    
    {"id": "data_cleaning_quiz_5", "questionText": "Which of the following is a potential problem with using mean imputation for missing values?", "type": "MCQ", "options": ["It reduces the variance in the data", "It increases the computational complexity", "It always introduces extreme values", "It requires domain expertise"], "correctAnswer": "It reduces the variance in the data", "explanation": "Mean imputation replaces missing values with the average, which artificially reduces the variance in the data since multiple distinct values are replaced with the same value. This can lead to underestimated standard errors and overly confident statistical inferences.", "difficulty": "HARD"}
  ]
}