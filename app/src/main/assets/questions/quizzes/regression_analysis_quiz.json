{
  "topicId": "regression_analysis",
  "quizQuestions": [
    {"id": "regression_quiz_1", "questionText": "In a multiple regression model, what does the coefficient of determination (R²) represent?", "type": "MCQ", "options": ["The proportion of variance in the dependent variable explained by the model", "The correlation between the independent variables", "The slope of the regression line", "The probability that the model is correct"], "correctAnswer": "The proportion of variance in the dependent variable explained by the model", "explanation": "R² measures how well the model fits the data by indicating what percentage of the variation in the dependent variable is explained by the independent variables in the model. It ranges from 0 (no explanation) to 1 (perfect fit).", "difficulty": "EASY"},
    
    {"id": "regression_quiz_2", "questionText": "Which of the following indicates heteroscedasticity in a regression model?", "type": "MCQ", "options": ["A funnel or fan shape in the residual plot", "A straight line in the Q-Q plot", "A Durbin-Watson statistic close to 2", "A low p-value in the F-test"], "correctAnswer": "A funnel or fan shape in the residual plot", "explanation": "Heteroscedasticity (non-constant error variance) is often visible as a funnel, fan, or cone shape in a residual vs. predicted values plot, where the spread of residuals increases or decreases with the predicted values.", "difficulty": "MEDIUM"},
    
    {"id": "regression_quiz_3", "questionText": "Which regression technique is most appropriate for predicting whether a customer will churn (yes/no)?", "type": "MCQ", "options": ["Logistic regression", "Linear regression", "Ridge regression", "Polynomial regression"], "correctAnswer": "Logistic regression", "explanation": "Logistic regression is designed for binary outcome prediction problems, making it appropriate for predicting customer churn (yes/no). It models the probability of the 'yes' outcome using the logistic function, keeping predictions between 0 and 1.", "difficulty": "EASY"},
    
    {"id": "regression_quiz_4", "questionText": "What problem does multicollinearity create in multiple regression models?", "type": "MCQ", "options": ["Unstable and unreliable coefficient estimates", "Biased predictions", "Decreased overall model fit (R²)", "Increased computational time"], "correctAnswer": "Unstable and unreliable coefficient estimates", "explanation": "Multicollinearity (high correlation between independent variables) makes it difficult to determine the individual effect of each predictor, resulting in unstable coefficient estimates with high standard errors. The overall model predictions may still be accurate, but individual coefficient interpretations become unreliable.", "difficulty": "MEDIUM"},
    
    {"id": "regression_quiz_5", "questionText": "Which regularization technique can perform automatic feature selection by setting some coefficients exactly to zero?", "type": "MCQ", "options": ["Lasso regression", "Ridge regression", "Ordinary Least Squares", "Principal Component Regression"], "correctAnswer": "Lasso regression", "explanation": "Lasso (Least Absolute Shrinkage and Selection Operator) uses an L1 penalty that can drive some coefficients exactly to zero, effectively removing those features from the model. This property makes Lasso useful for feature selection, especially in high-dimensional datasets.", "difficulty": "HARD"}
  ]
}